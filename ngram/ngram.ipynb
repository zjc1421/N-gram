{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Author: Jim Zhou"
      ],
      "metadata": {
        "id": "XTc1l9-NokTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.1 Import Dataset"
      ],
      "metadata": {
        "id": "wID1Dxyl5KfS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9dstDk_oN1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d371510b-d943-4dce-fffa-6820a0e2ad0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# 1.1 Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import math\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "dev = open(\"1b_benchmark.dev.tokens\", \"r\")\n",
        "test = open(\"1b_benchmark.test.tokens\", \"r\")\n",
        "train = open(\"1b_benchmark.train.tokens\", \"r\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd drive/MyDrive/447_HW1/"
      ],
      "metadata": {
        "id": "Wja4oIGB04Ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replace words to unk in input word_count\n",
        "# return a set contains all the words needed to convert to unk\n",
        "def filter_unk(word_count, freq):\n",
        "  word_to_unk = set()\n",
        "  unk_count = 0\n",
        "  for word in word_count.keys():\n",
        "    if word_count[word] < freq:\n",
        "      word_to_unk.add(word)\n",
        "      unk_count += word_count[word]\n",
        "  word_count[\"unk\"] = unk_count\n",
        "  for word in word_to_unk:\n",
        "    word_count.pop(word)\n",
        "  return word_to_unk"
      ],
      "metadata": {
        "id": "u3wRZUrW5__j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count for single word\n",
        "def count_for_unitoken(lines):\n",
        "  word_count_dict = {}\n",
        "  for line in lines:\n",
        "    cleaned_line = line.strip()\n",
        "    words = cleaned_line.split()\n",
        "\n",
        "    for word in words:\n",
        "      word_count_dict[word] = word_count_dict.get(word, 0) + 1\n",
        "    word_count_dict[\"<STOP>\"] = word_count_dict.get(\"<STOP>\", 0) + 1\n",
        "  return word_count_dict"
      ],
      "metadata": {
        "id": "Ya--1r5v9VWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count for bigram token\n",
        "def count_for_bitoken(lines, list_of_unk):\n",
        "  bitoken_count_dict = {}\n",
        "  for line in lines:\n",
        "    pre_word = \"<START>\"\n",
        "    cleaned_line = line.strip()\n",
        "    words = cleaned_line.split()\n",
        "    words.append(\"<STOP>\")\n",
        "\n",
        "    for word in words:\n",
        "      if word in list_of_unk:\n",
        "        word = \"unk\"\n",
        "      bitoken_count_dict[(pre_word, word)] = bitoken_count_dict.get((pre_word, word), 0) + 1\n",
        "      pre_word = word\n",
        "  return bitoken_count_dict"
      ],
      "metadata": {
        "id": "IadViNGdQxtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count for trigram token\n",
        "def count_for_tritoken(lines, list_of_unk):\n",
        "  tritoken_count_dict = {}\n",
        "  for line in lines:\n",
        "    pre_token = (\"<START>\", \"<START>\")\n",
        "    cleaned_line = line.strip()\n",
        "    words = cleaned_line.split()\n",
        "    words.append(\"<STOP>\")\n",
        "\n",
        "    for word in words:\n",
        "      if word in list_of_unk:\n",
        "        word = \"unk\"\n",
        "      tritoken_count_dict[(pre_token, word)] = tritoken_count_dict.get((pre_token, word), 0) + 1\n",
        "      pre_token = (pre_token[1], word)\n",
        "  return tritoken_count_dict"
      ],
      "metadata": {
        "id": "M4q5KvzuoPLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unigram(lines, K, freq):\n",
        "  word_count = count_for_unitoken(lines)\n",
        "  word_to_unk = filter_unk(word_count, freq)\n",
        "  total_word = 0\n",
        "\n",
        "  for word in word_count:\n",
        "    total_word += word_count[word]\n",
        "\n",
        "  token_prob = {}\n",
        "  for word in word_count.keys():\n",
        "    token_prob[word] = (word_count[word] / total_word, (word_count[word] + K) / (total_word + (K * len(word_count))))\n",
        "  return token_prob"
      ],
      "metadata": {
        "id": "n41xWvFp5N-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_unigram_prob(model, word, smooth):\n",
        "  return model[word][1] if smooth else model[word][0]"
      ],
      "metadata": {
        "id": "WxAVDidM2MqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bigram(lines, K, freq):\n",
        "  word_count = count_for_unitoken(lines)\n",
        "  word_to_unk = filter_unk(word_count, freq)\n",
        "\n",
        "  token_prob = defaultdict(dict)\n",
        "  bigram_count = count_for_bitoken(lines, word_to_unk)\n",
        "  for line in lines:\n",
        "    pre_word = \"<START>\"\n",
        "    cleaned_line = line.strip()\n",
        "    words = cleaned_line.split()\n",
        "    words.append(\"<STOP>\")\n",
        "\n",
        "    for word in words:\n",
        "      if word in word_to_unk:\n",
        "        word = \"unk\"\n",
        "      if pre_word == \"<START>\":\n",
        "        token_prob[pre_word][word] = (bigram_count[(pre_word, word)] / len(lines), (bigram_count[(pre_word, word)] + K) / (len(lines) + (K * len(word_count))))\n",
        "      else:\n",
        "        token_prob[pre_word][word] = (bigram_count[(pre_word, word)] / word_count[pre_word], (bigram_count[(pre_word, word)] + K) / (word_count[pre_word] + (K * len(word_count))))\n",
        "      pre_word = word\n",
        "  return token_prob"
      ],
      "metadata": {
        "id": "rxbToScOzhTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bigram_prob(K, size, model, pre_word, pre_word_freq, word, smooth):\n",
        "  return model[pre_word].get(word, (0, K / (pre_word_freq + K * size)))[1] if smooth else model[pre_word].get(word, (0, 0))[0]"
      ],
      "metadata": {
        "id": "dQjX9S2vNtE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trigram(lines, big_mode, K, freq):\n",
        "  word_count = count_for_unitoken(lines)\n",
        "  word_to_unk = filter_unk(word_count, freq)\n",
        "  bigram_count = count_for_bitoken(lines, word_to_unk)\n",
        "  trigram_count = count_for_tritoken(lines, word_to_unk)\n",
        "\n",
        "  token_prob = defaultdict(dict)\n",
        "  for line in lines:\n",
        "    pre_token = (\"<START>\", \"<START>\")\n",
        "    cleaned_line = line.strip()\n",
        "    words = cleaned_line.split()\n",
        "    words.append(\"<STOP>\")\n",
        "\n",
        "    for word in words:\n",
        "      if word in word_to_unk:\n",
        "        word = \"unk\"\n",
        "      if pre_token == (\"<START>\", \"<START>\"):\n",
        "        token_prob[pre_token][word] = (trigram_count[(pre_token, word)] / len(lines), (trigram_count[(pre_token, word)] + K) / (len(lines) + (K * len(word_count))))\n",
        "      else:\n",
        "        token_prob[pre_token][word] = (trigram_count[(pre_token, word)] / bigram_count[pre_token], (trigram_count[(pre_token, word)] + K) / (bigram_count[pre_token] + (K * len(word_count))))\n",
        "      pre_token = (pre_token[1], word)\n",
        "  return token_prob"
      ],
      "metadata": {
        "id": "XnHJqQ4dqYOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_trigram_prob(K, size, model, token, token_freq, smooth, word):\n",
        "  return model[token].get(word, (0, K / (token_freq + K * size)))[1] if smooth else model[token].get(word, (0, 0))[0]"
      ],
      "metadata": {
        "id": "QcZLKTVtRev2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_unk(word_count, vocab):\n",
        "  word_to_unk = set()\n",
        "  unk_count = 0\n",
        "  for word in word_count.keys():\n",
        "    if word not in vocab:\n",
        "      word_to_unk.add(word)\n",
        "      unk_count += word_count[word]\n",
        "  word_count[\"unk\"] = unk_count\n",
        "  for word in word_to_unk:\n",
        "    word_count.pop(word)\n",
        "  return word_to_unk"
      ],
      "metadata": {
        "id": "wxn3IL3646-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_interpolated_prob(K, size, pool, uni_model, bi_model, tri_model, unigram_freq, bigram_freq, spec_freq, pre_token, word, smooth):\n",
        "  uni_prob = uni_model[word][1] if smooth else uni_model[word][0]\n",
        "\n",
        "  if pre_token[1] == \"<START>\":\n",
        "    pre_word_freq = spec_freq\n",
        "  else:\n",
        "    pre_word_freq = unigram_freq[pre_token[1]]\n",
        "  bi_prob = get_bigram_prob(K, size, bi_model, pre_token[1], pre_word_freq, word, False)\n",
        "\n",
        "  tri_prob = -1\n",
        "  if pre_token not in tri_model:\n",
        "    tri_prob = 0\n",
        "  else:\n",
        "    pre_token_freq = -1\n",
        "    if pre_token == (\"<START>\", \"<START>\"):\n",
        "      pre_token_freq = spec_freq\n",
        "    else:\n",
        "      pre_token_freq = bigram_freq.get(pre_token, 0)\n",
        "    tri_prob = get_trigram_prob(K, size, tri_model, pre_token, pre_token_freq, smooth, word)\n",
        "\n",
        "  interp_prob = (pool[0] * uni_prob + pool[1] * bi_prob + pool[2] * tri_prob)\n",
        "  return interp_prob"
      ],
      "metadata": {
        "id": "FHHlygIDvKUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perplexity_interpolated(K, train_corpus, vocab, size, pool, uni_model, bi_model, tri_model, unigram_freq, bigram_freq, spec_freq, smooth):\n",
        "  perplexity = 0\n",
        "  word_count = count_for_unitoken(train_corpus)\n",
        "  process_unk(word_count, vocab)\n",
        "  for line in train_corpus:\n",
        "    log_sum = 0\n",
        "    cleaned_line = line.strip()\n",
        "    words = cleaned_line.split()\n",
        "    words.append(\"<STOP>\")\n",
        "\n",
        "    pre_token = (\"<START>\", \"<START>\")\n",
        "    for word in words:\n",
        "      if word not in vocab:\n",
        "        word = \"unk\"\n",
        "      log_sum += math.log(get_interpolated_prob(K, size, pool, uni_model, bi_model, tri_model, unigram_freq, bigram_freq, spec_freq, pre_token, word, smooth))\n",
        "      pre_token = (pre_token[1], word)\n",
        "    perplexity += math.exp(-log_sum / len(words))\n",
        "  perplexity /= len(train_corpus)\n",
        "  return perplexity"
      ],
      "metadata": {
        "id": "V58yg_bevIY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# K -> smoothing variable\n",
        "# N -> perform perplexity on which model (e.g. unigram for 1, bigram for 2 ...)\n",
        "# input_model -> model to evaluate\n",
        "# train_corpus -> evaluation dataset\n",
        "# vocab -> vocab of input model\n",
        "# unigram_freq -> a dict store the unigram token frequency\n",
        "# bigram_freq -> a dict store the bigram token frequency\n",
        "# sepec_freq -> a dict store the special token frequency\n",
        "#    ---> \"<START>\" for unigram_freq and (\"<START>\", \"<START>\") for bigram_freq\n",
        "def perplexity_laplace(K, N, input_model, train_corpus, vocab, unigram_freq, bigram_freq, spec_freq):\n",
        "  perplexity = 0\n",
        "  word_count = count_for_unitoken(train_corpus)\n",
        "  process_unk(word_count, vocab)\n",
        "\n",
        "  if N == 1:\n",
        "    for line in train_corpus:\n",
        "      log_sum = 0\n",
        "      cleaned_line = line.strip()\n",
        "      words = cleaned_line.split()\n",
        "      words.append(\"<STOP>\")\n",
        "\n",
        "      for word in words:\n",
        "        if word not in vocab:\n",
        "          word = \"unk\"\n",
        "        if word in input_model:\n",
        "          log_sum += math.log(input_model[word][1])\n",
        "      perplexity += math.exp(-log_sum / len(words))\n",
        "    perplexity /= len(train_corpus)\n",
        "    return perplexity\n",
        "\n",
        "  if N == 2:\n",
        "    for line in train_corpus:\n",
        "      log_sum = 0\n",
        "      cleaned_line = line.strip()\n",
        "      words = cleaned_line.split()\n",
        "      words.append(\"<STOP>\")\n",
        "\n",
        "      pre_word = \"<START>\"\n",
        "      for word in words:\n",
        "        if word not in vocab:\n",
        "          word = \"unk\"\n",
        "        pre_word_freq = -1\n",
        "        if pre_word == \"<START>\":\n",
        "          pre_word_freq = spec_freq\n",
        "        else:\n",
        "          pre_word_freq = unigram_freq[pre_word]\n",
        "        log_sum += math.log(get_bigram_prob(K, len(unigram_freq), input_model, pre_word, pre_word_freq, word, True))\n",
        "        pre_word = word\n",
        "      perplexity += math.exp(-log_sum / len(words))\n",
        "    perplexity /= len(train_corpus)\n",
        "    return perplexity\n",
        "\n",
        "  if N == 3:\n",
        "    for line in train_corpus:\n",
        "      log_sum = 0\n",
        "      cleaned_line = line.strip()\n",
        "      words = cleaned_line.split()\n",
        "      words.append(\"<STOP>\")\n",
        "\n",
        "      pre_token = (\"<START>\", \"<START>\")\n",
        "      for word in words:\n",
        "        if word not in vocab:\n",
        "          word = \"unk\"\n",
        "        token_freq = -1\n",
        "        if pre_token == (\"<START>\", \"<START>\"):\n",
        "          token_freq = spec_freq\n",
        "        else:\n",
        "          token_freq = bigram_freq.get(pre_token, 0)\n",
        "        log_sum += math.log(get_trigram_prob(K, len(unigram_freq), input_model, pre_token, token_freq, True, word))\n",
        "        pre_token = (pre_token[1], word)\n",
        "      perplexity += math.exp(-log_sum / len(words))\n",
        "    perplexity /= len(train_corpus)\n",
        "    return perplexity"
      ],
      "metadata": {
        "id": "JNQi6nDJeY6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perplexity(K, N, input_model, train_corpus, vocab, unigram_freq, bigram_freq, spec_freq):\n",
        "  perplexity = 0\n",
        "  word_count = count_for_unitoken(train_corpus)\n",
        "  process_unk(word_count, vocab)\n",
        "  if \"Resp\" in vocab:\n",
        "    print(\"Resp is in vocab\")\n",
        "\n",
        "  if N == 1:\n",
        "    for line in train_corpus:\n",
        "      log_sum = 0\n",
        "      cleaned_line = line.strip()\n",
        "      words = cleaned_line.split()\n",
        "      words.append(\"<STOP>\")\n",
        "\n",
        "      for word in words:\n",
        "        if word not in vocab:\n",
        "          word = \"unk\"\n",
        "        log_sum += math.log(input_model[word][0])\n",
        "      perplexity += math.exp(-log_sum / len(words))\n",
        "    perplexity /= len(train_corpus)\n",
        "    return perplexity\n",
        "\n",
        "  if N == 2:\n",
        "    for line in train_corpus:\n",
        "      log_sum = 0\n",
        "      cleaned_line = line.strip()\n",
        "      words = cleaned_line.split()\n",
        "      words.append(\"<STOP>\")\n",
        "\n",
        "      pre_word = \"<START>\"\n",
        "      for word in words:\n",
        "        if word not in vocab:\n",
        "          word = \"unk\"\n",
        "        try:\n",
        "          pre_word_freq = -1\n",
        "          if pre_word == \"<START>\":\n",
        "            pre_word_freq = spec_freq\n",
        "          else:\n",
        "            pre_word_freq = unigram_freq[pre_word]\n",
        "          log_sum += math.log(get_bigram_prob(K, len(unigram_freq), input_model, pre_word, pre_word_freq, word, False))\n",
        "        except:\n",
        "          return float('inf')\n",
        "        pre_word = word\n",
        "      perplexity += math.exp(-log_sum / len(words))\n",
        "    perplexity /= len(train_corpus)\n",
        "    return perplexity\n",
        "\n",
        "  if N == 3:\n",
        "    for line in train_corpus:\n",
        "      log_sum = 0\n",
        "      cleaned_line = line.strip()\n",
        "      words = cleaned_line.split()\n",
        "      words.append(\"<STOP>\")\n",
        "\n",
        "      pre_token = (\"<START>\", \"<START>\")\n",
        "      for word in words:\n",
        "        if word not in vocab:\n",
        "          word = \"unk\"\n",
        "        try:\n",
        "          token_freq = -1\n",
        "          if pre_token == (\"<START>\", \"<START>\"):\n",
        "            token_freq = spec_freq\n",
        "          else:\n",
        "            token_freq = bigram_freq.get(pre_token, 0)\n",
        "          log_sum += math.log(get_trigram_prob(K, len(unigram_freq), input_model, pre_token, token_freq, False, word))\n",
        "        except:\n",
        "          return float('inf')\n",
        "        pre_token = (pre_token[1], word)\n",
        "      perplexity += math.exp(-log_sum / len(words))\n",
        "    perplexity /= len(train_corpus)\n",
        "    return perplexity"
      ],
      "metadata": {
        "id": "gkdHyJnwYlMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_interp_fixed(var):\n",
        "  v1 = random.uniform(-var, var)\n",
        "  num1 = 0.1 + v1\n",
        "  v2 = random.uniform(-var, var)\n",
        "  num2 = 0.3 + v2\n",
        "  num3 = 1 - num1 - num2\n",
        "\n",
        "  return [num1, num2, num3]"
      ],
      "metadata": {
        "id": "fCe7b839K65g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_interp_rand():\n",
        "  num1 = random.uniform(0, 1)\n",
        "  num2 = random.uniform(0, 1 - num1)\n",
        "  num3 = 1 - num1 - num2\n",
        "\n",
        "  return [num1, num2, num3]"
      ],
      "metadata": {
        "id": "bdGPc1Bh25Rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.2 (a)\n",
        "In general, the process involves several steps:\n",
        "\n",
        "Firstly, the training file is processed by reading it and splitting it into individual tokens.\n",
        "\n",
        "Secondly, the frequency of each token and the type of token are counted.\n",
        "\n",
        "Thirdly, out-of-vocabulary (OOV) words are converted into \"unk\" and the token frequency is updated accordingly.\n",
        "\n",
        "For the unigram model, the probability of each word/unigram token is calculated using the equation: word frequency / total number of words. I use dict to store my unigram model: word -> prob\n",
        "\n",
        "For the bigram model, the bigram token frequency is also calculated. The probability of each bigram token is then calculated using the equation: bigram token frequency of (pre_word, word) / word frequency. I use 2D dict to store my bigram model: pre_word -> (word -> prob)\n",
        "\n",
        "For the trigram model, the trigram token frequency is also calculated. The probability of each trigram token is then calculated using the equation: trigram token frequency of (pre_pre_word, pre_word, word) / bigram token frequency of (pre_word, word). I use 2D dict to store my trigram model: pre_token -> (word -> prob)\n",
        "\n",
        "In the implementation, both smoothed and unsmoothed probabilities are stored for each token."
      ],
      "metadata": {
        "id": "gL2OWI-Ps-fZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3 (b)\n",
        "Step 1, I computed the perplexity for each line in the corpus. To calculate the perplexity for a line, I simply just followed the equation -- exponential of the average cross-entropy: I summed the logarithm of the probabilities of each unigram/bigram/trigram token, then negated this sum and divided it by the total number of words in the line. Finally, I exponentiated the result to obtain the line perplexity.\n",
        "\n",
        "Step 2, I aggregated the perplexities of all the lines and divided the sum by the total number of lines. This yielded the overall perplexity we sought."
      ],
      "metadata": {
        "id": "aEvffw_F3mV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "K = 1\n",
        "unk = 3\n",
        "train_f = train.readlines()\n",
        "dev_f = dev.readlines()\n",
        "test_f = test.readlines()\n",
        "spec_freq = len(train_f)\n",
        "unigram_freq = count_for_unitoken(train_f)\n",
        "unk_list = filter_unk(unigram_freq, unk)\n",
        "vocab = unigram_freq.keys()\n",
        "size = len(vocab)\n",
        "bigram_freq = count_for_bitoken(train_f, unk_list)\n",
        "unigram_model = unigram(train_f, K, unk)\n",
        "bigram_model = bigram(train_f, K, unk)\n",
        "trigram_model = trigram(train_f, bigram_model, K, unk)\n",
        "print(\"perplexity of unigram on train without smoothing: \", perplexity(K, 1, unigram_model, train_f, vocab, unigram_freq, bigram_freq, spec_freq))\n",
        "print(\"perplexity of bigram on train without smoothing:  \", perplexity(K, 2, bigram_model, train_f, vocab, unigram_freq, bigram_freq, spec_freq))\n",
        "print(\"perplexity of trigram on train without smoothing: \", perplexity(K, 3, trigram_model, train_f, vocab, unigram_freq, bigram_freq, spec_freq))\n",
        "print(\"perplexity of unigram on train with smoothing: \", perplexity_laplace(K, 1, unigram_model, train_f, vocab, unigram_freq, bigram_freq, spec_freq))\n",
        "print(\"perplexity of bigram on train with smoothing:  \", perplexity_laplace(K, 2, bigram_model, train_f, vocab, unigram_freq, bigram_freq, spec_freq))\n",
        "print(\"perplexity of trigram on train with smoothing: \", perplexity_laplace(K, 3, trigram_model, train_f, vocab, unigram_freq, bigram_freq, spec_freq))"
      ],
      "metadata": {
        "id": "cbduiXiVTq_4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af0a0324-e382-4e2d-abf2-580737db831a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perplexity of unigram on train without smoothing:  1080.3685030258303\n",
            "perplexity of bigram on train without smoothing:   77.98885904992798\n",
            "perplexity of trigram on train without smoothing:  8.548193804854302\n",
            "perplexity of unigram on train with smoothing:  1078.0014775956308\n",
            "perplexity of bigram on train with smoothing:   1557.0771005743309\n",
            "perplexity of trigram on train with smoothing:  6420.998230372765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"perplexity of unigram on dev without smoothing: \", perplexity(K, 1, unigram_model, dev_f, vocab, unigram_freq, bigram_freq, spec_freq))\n",
        "print(\"perplexity of bigram on dev without smoothing:  \", perplexity(K, 2, bigram_model, dev_f, vocab, unigram_freq, bigram_freq, spec_freq))\n",
        "print(\"perplexity of trigram on dev without smoothing: \", perplexity(K, 3, trigram_model, dev_f, vocab, unigram_freq, bigram_freq, spec_freq))\n",
        "print(\"perplexity of unigram on dev with smoothing: \", perplexity_laplace(K, 1, unigram_model, dev_f, vocab, unigram_freq, bigram_freq, spec_freq))\n",
        "print(\"perplexity of bigram on dev with smoothing:  \", perplexity_laplace(K, 2, bigram_model, dev_f, vocab, unigram_freq, bigram_freq, spec_freq))\n",
        "print(\"perplexity of trigram on dev with smoothing: \", perplexity_laplace(K, 3, trigram_model, dev_f, vocab, unigram_freq, bigram_freq, spec_freq))"
      ],
      "metadata": {
        "id": "-65qh5nLGHbS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50753830-8cab-4828-88d9-dcae1032c23b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perplexity of unigram on dev without smoothing:  985.5580004902903\n",
            "perplexity of bigram on dev without smoothing:   inf\n",
            "perplexity of trigram on dev without smoothing:  inf\n",
            "perplexity of unigram on dev with smoothing:  985.3139714750719\n",
            "perplexity of bigram on dev with smoothing:   1866.2182648946334\n",
            "perplexity of trigram on dev with smoothing:  10184.99583886916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"perplexity of unigram on test without smoothing: \", perplexity(K, 1, unigram_model, test_f, vocab, unigram_freq, bigram_freq, spec_freq))\n",
        "print(\"perplexity of bigram on test without smoothing:  \", perplexity(K, 2, bigram_model, test_f, vocab, unigram_freq, bigram_freq, spec_freq))\n",
        "print(\"perplexity of trigram on test without smoothing: \", perplexity(K, 3, trigram_model, test_f, vocab, unigram_freq, bigram_freq, spec_freq))\n",
        "print(\"perplexity of unigram on test with smoothing: \", perplexity_laplace(K, 1, unigram_model, test_f, vocab, unigram_freq, bigram_freq, spec_freq))\n",
        "print(\"perplexity of bigram on test with smoothing:  \", perplexity_laplace(K, 2, bigram_model, test_f, vocab, unigram_freq, bigram_freq, spec_freq))\n",
        "print(\"perplexity of trigram on test with smoothing: \", perplexity_laplace(K, 3, trigram_model, test_f, vocab, unigram_freq, bigram_freq, spec_freq))"
      ],
      "metadata": {
        "id": "hoPV0yBETu18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebe3ef46-34cf-4b02-ebed-e8a81888f3d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perplexity of unigram on test without smoothing:  998.7254932134222\n",
            "perplexity of bigram on test without smoothing:   inf\n",
            "perplexity of trigram on test without smoothing:  inf\n",
            "perplexity of unigram on test with smoothing:  998.247018669925\n",
            "perplexity of bigram on test with smoothing:   1864.0726257705326\n",
            "perplexity of trigram on test with smoothing:  10160.802419885664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.2 (c)\n",
        "\n",
        "The potential downside of Laplace smoothing lies in its potential to overestimate probabilities. By uniformly adding 1 to each token during probability calculation, there's a risk of artificially inflating the probabilities of rare events, thereby leading to biased estimates. For instance, in my trigram model, the probability of the trigram token ('\\<START>', '\\<START>', 'Nursing') initially stood at a minimum value of 1.6252234682268813e-05. However, after applying Laplace smoothing, the probability increased to 2.2693232877955795e-05, indicating a potential bias in estimation.\n",
        "\n",
        "Further evidence can be observed in the graph depicting the relationship between k values and the perplexity (PPL) of N-gram models with smoothing, as illustrated in the subsequent section 1.2 (d). In this graph, both bigram and trigram models exhibit an increase in PPL with higher k values, particularly noticeable in the case of the trigram model. This trend provides additional support for the potential limitations of Laplace smoothing in certain contexts."
      ],
      "metadata": {
        "id": "EetKpGqIo7XG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_prob_unsmooth = 1\n",
        "min_tri_token = \"\"\n",
        "for token in trigram_model:\n",
        "  for word, prob in trigram_model[token].items():\n",
        "    if min_prob_unsmooth > prob[0]:\n",
        "      min_prob_unsmooth = prob[0]\n",
        "      min_tri_token = (token[0], token[1], word)\n",
        "print(\"The trigram token with minmun unsmooth probability is: \", min_tri_token)\n",
        "print(\"It's unsmooth probability is : \", min_prob_unsmooth)\n",
        "print(\"It's smooth probability is : \", trigram_model[(min_tri_token[0], min_tri_token[1])][min_tri_token[2]][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hqfe3LWPmHMV",
        "outputId": "8b7bb3f7-47fe-45a7-d070-97299feb136e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The trigram token with minmun unsmooth probability is:  ('<START>', '<START>', 'Nursing')\n",
            "It's unsmooth probability is :  1.6252234682268813e-05\n",
            "It's smooth probability is :  2.2693232877955795e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.2 (d)\n",
        "The perplexity score increases as k increases."
      ],
      "metadata": {
        "id": "j_FG2m3A-_X1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "K1 = random.random()\n",
        "K2 = random.random()\n",
        "K3 = random.random()\n",
        "unigram_model_K1 = unigram(train_f, K1, unk)\n",
        "bigram_model_K1 = bigram(train_f, K1, unk)\n",
        "trigram_model_K1 = trigram(train_f, bigram_model, K1, unk)\n",
        "\n",
        "unigram_model_K2 = unigram(train_f, K2, unk)\n",
        "bigram_model_K2 = bigram(train_f, K2, unk)\n",
        "trigram_model_K2 = trigram(train_f, bigram_model, K2, unk)\n",
        "\n",
        "unigram_model_K3 = unigram(train_f, K3, unk)\n",
        "bigram_model_K3 = bigram(train_f, K3, unk)\n",
        "trigram_model_K3 = trigram(train_f, bigram_model, K3, unk)"
      ],
      "metadata": {
        "id": "hfG4FXvlCcp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uni_ppl_unsmooth_K1 = perplexity(K1, 1, unigram_model_K1, test_f, vocab, unigram_freq, bigram_freq, spec_freq)\n",
        "bi_ppl_unsmooth_K1 = perplexity(K1, 2, bigram_model_K1, test_f, vocab, unigram_freq, bigram_freq, spec_freq)\n",
        "tri_ppl_unsmooth_K1 = perplexity(K1, 3, trigram_model_K1, test_f, vocab, unigram_freq, bigram_freq, spec_freq)\n",
        "print(f\"perplexity of unigram on test without smoothing for k = {K1}: \", uni_ppl_unsmooth_K1)\n",
        "print(f\"perplexity of bigram on test without smoothing for k = {K1}: \", bi_ppl_unsmooth_K1)\n",
        "print(f\"perplexity of trigram on test without smoothing for k = {K1}: \", tri_ppl_unsmooth_K1)\n",
        "uni_ppl_smooth_K1 = perplexity_laplace(K1, 1, unigram_model_K1, test_f, vocab, unigram_freq, bigram_freq, spec_freq)\n",
        "bi_ppl_smooth_K1 = perplexity_laplace(K1, 2, bigram_model_K1, test_f, vocab, unigram_freq, bigram_freq, spec_freq)\n",
        "tri_ppl_smooth_K1 = perplexity_laplace(K1, 3, trigram_model_K1, test_f, vocab, unigram_freq, bigram_freq, spec_freq)\n",
        "print(f\"perplexity of unigram on test with smoothing for k = {K1}: \", uni_ppl_smooth_K1)\n",
        "print(f\"perplexity of bigram on test with smoothing for k = {K1}: \", bi_ppl_smooth_K1)\n",
        "print(f\"perplexity of trigram on test with smoothing for k = {K1}: \", tri_ppl_smooth_K1)\n",
        "\n",
        "uni_ppl_unsmooth_K2 = perplexity(K2, 1, unigram_model_K2, test_f, vocab, unigram_freq, bigram_freq, spec_freq)\n",
        "bi_ppl_unsmooth_K2 = perplexity(K2, 2, bigram_model_K2, test_f, vocab, unigram_freq, bigram_freq, spec_freq)\n",
        "tri_ppl_unsmooth_K2 = perplexity(K2, 3, trigram_model_K2, test_f, vocab, unigram_freq, bigram_freq, spec_freq)\n",
        "print(f\"perplexity of unigram on test without smoothing for k = {K2}: \", uni_ppl_unsmooth_K2)\n",
        "print(f\"perplexity of bigram on test without smoothing for k = {K2}: \", bi_ppl_unsmooth_K2)\n",
        "print(f\"perplexity of trigram on test without smoothing for k = {K2}: \", tri_ppl_unsmooth_K2)\n",
        "uni_ppl_smooth_K2 = perplexity_laplace(K2, 1, unigram_model_K2, test_f, vocab, unigram_freq, bigram_freq, spec_freq)\n",
        "bi_ppl_smooth_K2 = perplexity_laplace(K2, 2, bigram_model_K2, test_f, vocab, unigram_freq, bigram_freq, spec_freq)\n",
        "tri_ppl_smooth_K2 = perplexity_laplace(K2, 3, trigram_model_K2, test_f, vocab, unigram_freq, bigram_freq, spec_freq)\n",
        "print(f\"perplexity of unigram on test with smoothing for k = {K2}: \", uni_ppl_smooth_K2)\n",
        "print(f\"perplexity of bigram on test with smoothing for k = {K2}: \", bi_ppl_smooth_K2)\n",
        "print(f\"perplexity of trigram on test with smoothing for k = {K2}: \", tri_ppl_smooth_K2)\n",
        "\n",
        "uni_ppl_unsmooth_K3 = perplexity(K3, 1, unigram_model_K3, test_f, vocab, unigram_freq, bigram_freq, spec_freq)\n",
        "bi_ppl_unsmooth_K3 = perplexity(K3, 2, bigram_model_K3, test_f, vocab, unigram_freq, bigram_freq, spec_freq)\n",
        "tri_ppl_unsmooth_K3 = perplexity(K3, 3, trigram_model_K3, test_f, vocab, unigram_freq, bigram_freq, spec_freq)\n",
        "print(f\"perplexity of unigram on test without smoothing for k = {K3}: \", uni_ppl_unsmooth_K3)\n",
        "print(f\"perplexity of bigram on test without smoothing for k = {K3}: \", bi_ppl_unsmooth_K3)\n",
        "print(f\"perplexity of trigram on test without smoothing for k = {K3}: \", tri_ppl_unsmooth_K3)\n",
        "uni_ppl_smooth_K3 = perplexity_laplace(K3, 1, unigram_model_K3, test_f, vocab, unigram_freq, bigram_freq, spec_freq)\n",
        "bi_ppl_smooth_K3 = perplexity_laplace(K3, 2, bigram_model_K3, test_f, vocab, unigram_freq, bigram_freq, spec_freq)\n",
        "tri_ppl_smooth_K3 = perplexity_laplace(K3, 3, trigram_model_K3, test_f, vocab, unigram_freq, bigram_freq, spec_freq)\n",
        "print(f\"perplexity of unigram on test with smoothing for k = {K3}: \", uni_ppl_smooth_K3)\n",
        "print(f\"perplexity of bigram on test with smoothing for k = {K3}: \", bi_ppl_smooth_K3)\n",
        "print(f\"perplexity of trigram on test with smoothing for k = {K3}: \", tri_ppl_smooth_K3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kh79dytNNioR",
        "outputId": "2fcd88c6-f2a6-460f-92ba-bbd99cafffca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perplexity of unigram on test without smoothing for k = 0.9393556645493956:  998.7254932134222\n",
            "perplexity of bigram on test without smoothing for k = 0.9393556645493956:  inf\n",
            "perplexity of trigram on test without smoothing for k = 0.9393556645493956:  inf\n",
            "perplexity of unigram on test with smoothing for k = 0.9393556645493956:  998.2198038588477\n",
            "perplexity of bigram on test with smoothing for k = 0.9393556645493956:  1816.8204021647102\n",
            "perplexity of trigram on test with smoothing for k = 0.9393556645493956:  10004.98454917514\n",
            "perplexity of unigram on test without smoothing for k = 0.3604337412593719:  998.7254932134222\n",
            "perplexity of bigram on test without smoothing for k = 0.3604337412593719:  inf\n",
            "perplexity of trigram on test without smoothing for k = 0.3604337412593719:  inf\n",
            "perplexity of unigram on test with smoothing for k = 0.3604337412593719:  998.3047104612003\n",
            "perplexity of bigram on test with smoothing for k = 0.3604337412593719:  1257.853544741693\n",
            "perplexity of trigram on test with smoothing for k = 0.3604337412593719:  7857.035699630031\n",
            "perplexity of unigram on test without smoothing for k = 0.2716381391001915:  998.7254932134222\n",
            "perplexity of bigram on test without smoothing for k = 0.2716381391001915:  inf\n",
            "perplexity of trigram on test without smoothing for k = 0.2716381391001915:  inf\n",
            "perplexity of unigram on test with smoothing for k = 0.2716381391001915:  998.379304976322\n",
            "perplexity of bigram on test with smoothing for k = 0.2716381391001915:  1140.649310643902\n",
            "perplexity of trigram on test with smoothing for k = 0.2716381391001915:  7316.697858265984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k_list = [K1, K2, K3]\n",
        "uni_ppl_smooth_list = [uni_ppl_smooth_K1, uni_ppl_smooth_K2, uni_ppl_smooth_K3]\n",
        "bi_ppl_smooth_list = [bi_ppl_smooth_K1, bi_ppl_smooth_K2, bi_ppl_smooth_K3]\n",
        "tri_ppl_smooth_list = [tri_ppl_smooth_K1, uni_ppl_smooth_K2, uni_ppl_smooth_K3]\n",
        "\n",
        "plt.plot(k_list, uni_ppl_smooth_list, label='Unigram_PPL')\n",
        "plt.plot(k_list, bi_ppl_smooth_list, label='Bigram_PPL')\n",
        "plt.plot(k_list, tri_ppl_smooth_list, label='Trigram_PPL')\n",
        "\n",
        "plt.xlabel('K values')\n",
        "plt.ylabel('PPL with smooth')\n",
        "plt.title('Ngram model\\'s PPL changing with smoothing K')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "7flDPnzjdd4c",
        "outputId": "65fdfd37-1331-4442-86b5-6662553d9cdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.2 (e)\n",
        "In my training set, the unsmoothed versions for the unigram, bigram, and trigram models exhibit an expected pattern, where the trigram model (with the smallest perplexity (PPL) value) ideally performs better than the bigram model, which in turn performs better than the unigram model (with the largest PPL value). However, the situation is reversed in my smoothed versions, where the PPL of the unigram model is less than that of the bigram model, which is less than that of the trigram model. This discrepancy could serve as evidence of overestimation resulting from Laplace smoothing. In the trigram model, there are numerous rare trigram tokens with extremely low frequencies present in the corpus. Consequently, their probabilities could be significantly lower compared to the probabilities of unigram tokens in the unigram model. Thus, Laplace smoothing tends to overestimate the probabilities of these rare trigram tokens. This same phenomenon is observed in the smoothed versions for the development and test sets as well.\n",
        "\n",
        "For the development and test sets, the unsmoothed versions of the bigram and trigram models yield infinite values. This occurs because my bigram and trigram models lack probabilities for certain bigram and trigram tokens in the development and test sets, resulting in zero probabilities. Consequently, when calculating perplexity, attempting to take the logarithm of zero probability results in an infinite value."
      ],
      "metadata": {
        "id": "iNigpdmHByz1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3 (a)"
      ],
      "metadata": {
        "id": "nmUCEdIvEL4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opti_hyper = [0.531742, 0.420426, 0.047832]\n",
        "print(f\"My best combination of hyperparameters is: {opti_hyper[0]}, {opti_hyper[1]}, {opti_hyper[2]}\")\n",
        "interp_ppl_unsmooth = perplexity_interpolated(K, dev_f, vocab, size, opti_hyper, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, False)\n",
        "interp_ppl_smooth = perplexity_interpolated(K, dev_f, vocab, size, opti_hyper, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, True)\n",
        "print(f\"perplexity of interpolation on dev without smoothing for hyperparameters {opti_hyper[0]}, {opti_hyper[1]}, {opti_hyper[2]}:  \", interp_ppl_unsmooth)\n",
        "print(f\"perplexity of interpolation on dev with smoothing for hyperparameters {opti_hyper[0]}, {opti_hyper[1]}, {opti_hyper[2]}:  \", interp_ppl_smooth)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqbOv-vlPTnh",
        "outputId": "dc53d7ae-9925-4078-d0c9-05d7c7ca550e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My best combination of hyperparameters is: 0.531742, 0.420426, 0.047832\n",
            "perplexity of interpolation on dev without smoothing for hyperparameters 0.531742, 0.420426, 0.047832:   340.0894018669363\n",
            "perplexity of interpolation on dev with smoothing for hyperparameters 0.531742, 0.420426, 0.047832:   360.0779423181838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pool = [0.1, 0.3, 0.6]\n",
        "pool1 = gen_interp_fixed(0.1)\n",
        "pool2 = gen_interp_fixed(0.2)\n",
        "pool3 = gen_interp_rand()\n",
        "pool4 = gen_interp_rand()\n",
        "pool5 = gen_interp_rand()"
      ],
      "metadata": {
        "id": "jxUOzxJDXK61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interp_ppl_unsmooth = perplexity_interpolated(K, dev_f, vocab, size, pool, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, False)\n",
        "interp_ppl_smooth = perplexity_interpolated(K, dev_f, vocab, size, pool, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, True)\n",
        "print(f\"perplexity of interpolation on dev without smoothing for hyperparameters {pool[0]}, {pool[1]}, {pool[2]}:  \", interp_ppl_unsmooth)\n",
        "print(f\"perplexity of interpolation on dev with smoothing for hyperparameters {pool[0]}, {pool[1]}, {pool[2]}:  \", interp_ppl_smooth)\n",
        "interp_ppl_unsmooth = perplexity_interpolated(K, train_f, vocab, size, pool, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, False)\n",
        "interp_ppl_smooth = perplexity_interpolated(K, train_f, vocab, size, pool, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, True)\n",
        "print(f\"perplexity of interpolation on train without smoothing for hyperparameters {pool[0]}, {pool[1]}, {pool[2]}:  \", interp_ppl_unsmooth)\n",
        "print(f\"perplexity of interpolation on train with smoothing for hyperparameters {pool[0]}, {pool[1]}, {pool[2]}:  \", interp_ppl_smooth)\n",
        "\n",
        "interp_ppl_unsmooth = perplexity_interpolated(K, dev_f, vocab, size, pool1, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, False)\n",
        "interp_ppl_smooth = perplexity_interpolated(K, dev_f, vocab, size, pool1, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, True)\n",
        "print(f\"perplexity of interpolation on dev without smoothing for hyperparameters {pool1[0]}, {pool1[1]}, {pool1[2]}:  \", interp_ppl_unsmooth)\n",
        "print(f\"perplexity of interpolation on dev with smoothing for hyperparameters {pool1[0]}, {pool1[1]}, {pool1[2]}:  \", interp_ppl_smooth)\n",
        "interp_ppl_unsmooth = perplexity_interpolated(K, train_f, vocab, size, pool1, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, False)\n",
        "interp_ppl_smooth = perplexity_interpolated(K, train_f, vocab, size, pool1, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, True)\n",
        "print(f\"perplexity of interpolation on train without smoothing for hyperparameters {pool1[0]}, {pool1[1]}, {pool1[2]}:  \", interp_ppl_unsmooth)\n",
        "print(f\"perplexity of interpolation on train with smoothing for hyperparameters {pool1[0]}, {pool1[1]}, {pool1[2]}:  \", interp_ppl_smooth)\n",
        "\n",
        "interp_ppl_unsmooth = perplexity_interpolated(K, dev_f, vocab, size, pool2, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, False)\n",
        "interp_ppl_smooth = perplexity_interpolated(K, dev_f, vocab, size, pool2, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, True)\n",
        "print(f\"perplexity of interpolation on dev without smoothing for hyperparameters {pool2[0]}, {pool2[1]}, {pool2[2]}:  \", interp_ppl_unsmooth)\n",
        "print(f\"perplexity of interpolation on dev with smoothing for hyperparameters {pool2[0]}, {pool2[1]}, {pool2[2]}:  \", interp_ppl_smooth)\n",
        "interp_ppl_unsmooth = perplexity_interpolated(K, train_f, vocab, size, pool2, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, False)\n",
        "interp_ppl_smooth = perplexity_interpolated(K, train_f, vocab, size, pool2, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, True)\n",
        "print(f\"perplexity of interpolation on train without smoothing for hyperparameters {pool2[0]}, {pool2[1]}, {pool2[2]}:  \", interp_ppl_unsmooth)\n",
        "print(f\"perplexity of interpolation on train with smoothing for hyperparameters {pool2[0]}, {pool2[1]}, {pool2[2]}:  \", interp_ppl_smooth)\n",
        "\n",
        "interp_ppl_unsmooth = perplexity_interpolated(K, dev_f, vocab, size, pool3, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, False)\n",
        "interp_ppl_smooth = perplexity_interpolated(K, dev_f, vocab, size, pool3, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, True)\n",
        "print(f\"perplexity of interpolation on dev without smoothing for hyperparameters {pool3[0]}, {pool3[1]}, {pool3[2]}:  \", interp_ppl_unsmooth)\n",
        "print(f\"perplexity of interpolation on dev with smoothing for hyperparameters {pool3[0]}, {pool3[1]}, {pool3[2]}:  \", interp_ppl_smooth)\n",
        "interp_ppl_unsmooth = perplexity_interpolated(K, train_f, vocab, size, pool3, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, False)\n",
        "interp_ppl_smooth = perplexity_interpolated(K, train_f, vocab, size, pool3, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, True)\n",
        "print(f\"perplexity of interpolation on train without smoothing for hyperparameters {pool3[0]}, {pool3[1]}, {pool3[2]}:  \", interp_ppl_unsmooth)\n",
        "print(f\"perplexity of interpolation on train with smoothing for hyperparameters {pool3[0]}, {pool3[1]}, {pool3[2]}:  \", interp_ppl_smooth)\n",
        "\n",
        "interp_ppl_unsmooth = perplexity_interpolated(K, dev_f, vocab, size, pool4, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, False)\n",
        "interp_ppl_smooth = perplexity_interpolated(K, dev_f, vocab, size, pool4, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, True)\n",
        "print(f\"perplexity of interpolation on dev without smoothing for hyperparameters {pool4[0]}, {pool4[1]}, {pool4[2]}:  \", interp_ppl_unsmooth)\n",
        "print(f\"perplexity of interpolation on dev with smoothing for hyperparameters {pool4[0]}, {pool4[1]}, {pool4[2]}:  \", interp_ppl_smooth)\n",
        "interp_ppl_unsmooth = perplexity_interpolated(K, train_f, vocab, size, pool4, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, False)\n",
        "interp_ppl_smooth = perplexity_interpolated(K, train_f, vocab, size, pool4, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, True)\n",
        "print(f\"perplexity of interpolation on train without smoothing for hyperparameters {pool4[0]}, {pool4[1]}, {pool4[2]}:  \", interp_ppl_unsmooth)\n",
        "print(f\"perplexity of interpolation on train with smoothing for hyperparameters {pool4[0]}, {pool4[1]}, {pool4[2]}:  \", interp_ppl_smooth)\n",
        "\n",
        "interp_ppl_unsmooth = perplexity_interpolated(K, dev_f, vocab, size, pool5, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, False)\n",
        "interp_ppl_smooth = perplexity_interpolated(K, dev_f, vocab, size, pool5, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, True)\n",
        "print(f\"perplexity of interpolation on dev without smoothing for hyperparameters {pool5[0]}, {pool5[1]}, {pool5[2]}:  \", interp_ppl_unsmooth)\n",
        "print(f\"perplexity of interpolation on dev with smoothing for hyperparameters {pool5[0]}, {pool5[1]}, {pool5[2]}:  \", interp_ppl_smooth)\n",
        "interp_ppl_unsmooth = perplexity_interpolated(K, train_f, vocab, size, pool5, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, False)\n",
        "interp_ppl_smooth = perplexity_interpolated(K, train_f, vocab, size, pool5, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, True)\n",
        "print(f\"perplexity of interpolation on train without smoothing for hyperparameters {pool5[0]}, {pool5[1]}, {pool5[2]}:  \", interp_ppl_unsmooth)\n",
        "print(f\"perplexity of interpolation on train with smoothing for hyperparameters {pool5[0]}, {pool5[1]}, {pool5[2]}:  \", interp_ppl_smooth)"
      ],
      "metadata": {
        "id": "aU1CjbhC5ghQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "928f7248-09e0-40ef-abe3-c4ece2def7d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perplexity of interpolation on dev without smoothing for hyperparameters 0.1, 0.3, 0.6:   534.6931564837076\n",
            "perplexity of interpolation on dev with smoothing for hyperparameters 0.1, 0.3, 0.6:   585.5165835126105\n",
            "perplexity of interpolation on train without smoothing for hyperparameters 0.1, 0.3, 0.6:   11.883591542089817\n",
            "perplexity of interpolation on train with smoothing for hyperparameters 0.1, 0.3, 0.6:   192.95334611988721\n",
            "perplexity of interpolation on dev without smoothing for hyperparameters 0.14755282570936595, 0.21850442546834697, 0.6339427488222871:   517.2581011914799\n",
            "perplexity of interpolation on dev with smoothing for hyperparameters 0.14755282570936595, 0.21850442546834697, 0.6339427488222871:   625.4485806927094\n",
            "perplexity of interpolation on train without smoothing for hyperparameters 0.14755282570936595, 0.21850442546834697, 0.6339427488222871:   11.709528505659422\n",
            "perplexity of interpolation on train with smoothing for hyperparameters 0.14755282570936595, 0.21850442546834697, 0.6339427488222871:   236.3760873119985\n",
            "perplexity of interpolation on dev without smoothing for hyperparameters 0.17436318600033432, 0.21272670103405855, 0.6129101129656072:   493.3884937793069\n",
            "perplexity of interpolation on dev with smoothing for hyperparameters 0.17436318600033432, 0.21272670103405855, 0.6129101129656072:   608.7944541331\n",
            "perplexity of interpolation on train without smoothing for hyperparameters 0.17436318600033432, 0.21272670103405855, 0.6129101129656072:   12.058144071884813\n",
            "perplexity of interpolation on train with smoothing for hyperparameters 0.17436318600033432, 0.21272670103405855, 0.6129101129656072:   237.71481739511964\n",
            "perplexity of interpolation on dev without smoothing for hyperparameters 0.47512142959162307, 0.18925370967601468, 0.33562486073236225:   383.4987945773886\n",
            "perplexity of interpolation on dev with smoothing for hyperparameters 0.47512142959162307, 0.18925370967601468, 0.33562486073236225:   494.31649726338327\n",
            "perplexity of interpolation on train without smoothing for hyperparameters 0.47512142959162307, 0.18925370967601468, 0.33562486073236225:   19.49282473995578\n",
            "perplexity of interpolation on train with smoothing for hyperparameters 0.47512142959162307, 0.18925370967601468, 0.33562486073236225:   231.892618252011\n",
            "perplexity of interpolation on dev without smoothing for hyperparameters 0.4074959302238965, 0.1052622401583267, 0.4872418296177768:   447.0865761808299\n",
            "perplexity of interpolation on dev with smoothing for hyperparameters 0.4074959302238965, 0.1052622401583267, 0.4872418296177768:   640.1236128474551\n",
            "perplexity of interpolation on train without smoothing for hyperparameters 0.4074959302238965, 0.1052622401583267, 0.4872418296177768:   15.21971484699397\n",
            "perplexity of interpolation on train with smoothing for hyperparameters 0.4074959302238965, 0.1052622401583267, 0.4872418296177768:   327.7965751408414\n",
            "perplexity of interpolation on dev without smoothing for hyperparameters 0.5724210964883528, 0.3304194228831576, 0.09715948062848956:   347.1454862507856\n",
            "perplexity of interpolation on dev with smoothing for hyperparameters 0.5724210964883528, 0.3304194228831576, 0.09715948062848956:   385.6924539554744\n",
            "perplexity of interpolation on train without smoothing for hyperparameters 0.5724210964883528, 0.3304194228831576, 0.09715948062848956:   38.918394744601045\n",
            "perplexity of interpolation on train with smoothing for hyperparameters 0.5724210964883528, 0.3304194228831576, 0.09715948062848956:   162.5357732295005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "interp_ppl_unsmooth = perplexity_interpolated(K, test_f, vocab, size, pool, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, False)\n",
        "interp_ppl_smooth = perplexity_interpolated(K, test_f, vocab, size, pool, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, True)\n",
        "print(f\"perplexity of interpolation on test without smoothing for hyperparameters {pool[0]}, {pool[1]}, {pool[2]}:  \", interp_ppl_unsmooth)\n",
        "print(f\"perplexity of interpolation on test with smoothing for hyperparameters {pool[0]}, {pool[1]}, {pool[2]}:  \", interp_ppl_smooth)\n",
        "\n",
        "interp_ppl_unsmooth = perplexity_interpolated(K, test_f, vocab, size, pool1, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, False)\n",
        "interp_ppl_smooth = perplexity_interpolated(K, test_f, vocab, size, pool1, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, True)\n",
        "print(f\"perplexity of interpolation on test without smoothing for hyperparameters {pool1[0]}, {pool1[1]}, {pool1[2]}:  \", interp_ppl_unsmooth)\n",
        "print(f\"perplexity of interpolation on test with smoothing for hyperparameters {pool1[0]}, {pool1[1]}, {pool1[2]}:  \", interp_ppl_smooth)\n",
        "\n",
        "interp_ppl_unsmooth = perplexity_interpolated(K, test_f, vocab, size, pool2, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, False)\n",
        "interp_ppl_smooth = perplexity_interpolated(K, test_f, vocab, size, pool2, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, True)\n",
        "print(f\"perplexity of interpolation on test without smoothing for hyperparameters {pool2[0]}, {pool2[1]}, {pool2[2]}:  \", interp_ppl_unsmooth)\n",
        "print(f\"perplexity of interpolation on test with smoothing for hyperparameters {pool2[0]}, {pool2[1]}, {pool2[2]}:  \", interp_ppl_smooth)\n",
        "\n",
        "interp_ppl_unsmooth = perplexity_interpolated(K, test_f, vocab, size, pool3, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, False)\n",
        "interp_ppl_smooth = perplexity_interpolated(K, test_f, vocab, size, pool3, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, True)\n",
        "print(f\"perplexity of interpolation on test without smoothing for hyperparameters {pool3[0]}, {pool3[1]}, {pool3[2]}:  \", interp_ppl_unsmooth)\n",
        "print(f\"perplexity of interpolation on test with smoothing for hyperparameters {pool3[0]}, {pool3[1]}, {pool3[2]}:  \", interp_ppl_smooth)\n",
        "\n",
        "interp_ppl_unsmooth = perplexity_interpolated(K, test_f, vocab, size, pool4, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, False)\n",
        "interp_ppl_smooth = perplexity_interpolated(K, test_f, vocab, size, pool4, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, True)\n",
        "print(f\"perplexity of interpolation on test without smoothing for hyperparameters {pool4[0]}, {pool4[1]}, {pool4[2]}:  \", interp_ppl_unsmooth)\n",
        "print(f\"perplexity of interpolation on test with smoothing for hyperparameters {pool4[0]}, {pool4[1]}, {pool4[2]}:  \", interp_ppl_smooth)\n",
        "\n",
        "interp_ppl_unsmooth = perplexity_interpolated(K, test_f, vocab, size, pool5, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, False)\n",
        "interp_ppl_smooth = perplexity_interpolated(K, test_f, vocab, size, pool5, unigram_model, bigram_model, trigram_model, unigram_freq, bigram_freq, spec_freq, True)\n",
        "print(f\"perplexity of interpolation on test without smoothing for hyperparameters {pool5[0]}, {pool5[1]}, {pool5[2]}:  \", interp_ppl_unsmooth)\n",
        "print(f\"perplexity of interpolation on test with smoothing for hyperparameters {pool5[0]}, {pool5[1]}, {pool5[2]}:  \", interp_ppl_smooth)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ey3oy43BYECB",
        "outputId": "54a37a37-c52a-422d-8165-31d9ab4c1739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perplexity of interpolation on dev without smoothing for hyperparameters 0.1, 0.3, 0.6:   543.0683055194663\n",
            "perplexity of interpolation on dev with smoothing for hyperparameters 0.1, 0.3, 0.6:   594.6286705518622\n",
            "perplexity of interpolation on dev without smoothing for hyperparameters 0.14755282570936595, 0.21850442546834697, 0.6339427488222871:   523.8469791306345\n",
            "perplexity of interpolation on dev with smoothing for hyperparameters 0.14755282570936595, 0.21850442546834697, 0.6339427488222871:   633.3699879295476\n",
            "perplexity of interpolation on dev without smoothing for hyperparameters 0.17436318600033432, 0.21272670103405855, 0.6129101129656072:   499.27706305726906\n",
            "perplexity of interpolation on dev with smoothing for hyperparameters 0.17436318600033432, 0.21272670103405855, 0.6129101129656072:   616.1486606091681\n",
            "perplexity of interpolation on dev without smoothing for hyperparameters 0.47512142959162307, 0.18925370967601468, 0.33562486073236225:   386.8955500845346\n",
            "perplexity of interpolation on dev with smoothing for hyperparameters 0.47512142959162307, 0.18925370967601468, 0.33562486073236225:   499.4743058605743\n",
            "perplexity of interpolation on dev without smoothing for hyperparameters 0.4074959302238965, 0.1052622401583267, 0.4872418296177768:   451.11525133709574\n",
            "perplexity of interpolation on dev with smoothing for hyperparameters 0.4074959302238965, 0.1052622401583267, 0.4872418296177768:   646.7497659632966\n",
            "perplexity of interpolation on dev without smoothing for hyperparameters 0.5724210964883528, 0.3304194228831576, 0.09715948062848956:   350.4412832867883\n",
            "perplexity of interpolation on dev with smoothing for hyperparameters 0.5724210964883528, 0.3304194228831576, 0.09715948062848956:   389.74678438879806\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3 (c)\n",
        "Using only half of the training data could lead to a decrease in the perplexity of previously unseen data, primarily by lowering the risk of overfitting. When my model is trained on a large dataset and closely fits to it, it may struggle when faced with an unseen dataset that differs significantly from the training set. By using a smaller subset of the data, the model is less likely to overfit to specific patterns present in the training set, resulting in better generalization to new, unseen data.\n",
        "\n",
        "Perplexity:\n",
        "perplexity of unigram trained by whole file on test with smoothing:  998.247018669925\n",
        "\n",
        "perplexity of bigram trained by whole file on test with smoothing:   1864.0726257705326\n",
        "\n",
        "perplexity of trigram trained by whole file on test with smoothing:  10160.802419885664\n",
        "\n",
        "perplexity of unigram trained by half file on test with smoothing:  812.9105609540952\n",
        "\n",
        "perplexity of bigram trained by half file on test with smoothing:   1552.1012805741357\n",
        "\n",
        "perplexity of trigram trained by half file on test with smoothing:  7361.21230049765"
      ],
      "metadata": {
        "id": "vAdkX8Sg_AlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "half_train_f = train_f[:(len(train_f) // 2)]\n",
        "spec_freq_half = len(half_train_f)\n",
        "unigram_freq_half = count_for_unitoken(half_train_f)\n",
        "unk_list_half = filter_unk(unigram_freq_half, unk)\n",
        "vocab_half = unigram_freq_half.keys()\n",
        "size_half = len(vocab)\n",
        "bigram_freq_half = count_for_bitoken(half_train_f, unk_list)\n",
        "unigram_model_half = unigram(half_train_f, K, unk)\n",
        "bigram_model_half = bigram(half_train_f, K, unk)\n",
        "trigram_model_half = trigram(half_train_f, bigram_model_half, K, unk)"
      ],
      "metadata": {
        "id": "umUxi4S0-P_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"perplexity of unigram on test without smoothing: \", perplexity(K, 1, unigram_model_half, test_f, vocab_half, unigram_freq_half, bigram_freq_half, spec_freq_half))\n",
        "print(\"perplexity of bigram on test without smoothing:  \", perplexity(K, 2, bigram_model_half, test_f, vocab_half, unigram_freq_half, bigram_freq_half, spec_freq_half))\n",
        "print(\"perplexity of trigram on test without smoothing: \", perplexity(K, 3, trigram_model_half, test_f, vocab_half, unigram_freq_half, bigram_freq_half, spec_freq_half))\n",
        "print(\"perplexity of unigram on test with smoothing: \", perplexity_laplace(K, 1, unigram_model_half, test_f, vocab_half, unigram_freq_half, bigram_freq_half, spec_freq_half))\n",
        "print(\"perplexity of bigram on test with smoothing:  \", perplexity_laplace(K, 2, bigram_model_half, test_f, vocab_half, unigram_freq_half, bigram_freq_half, spec_freq_half))\n",
        "print(\"perplexity of trigram on test with smoothing: \", perplexity_laplace(K, 3, trigram_model_half, test_f, vocab_half, unigram_freq_half, bigram_freq_half, spec_freq_half))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpE_wM1A_5Ev",
        "outputId": "c99ce00b-56d4-4cad-fd4c-1b9c3bb055f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perplexity of unigram on test without smoothing:  812.6340218368663\n",
            "perplexity of bigram on test without smoothing:   inf\n",
            "perplexity of trigram on test without smoothing:  inf\n",
            "perplexity of unigram on test with smoothing:  812.9105609540952\n",
            "perplexity of bigram on test with smoothing:   1552.1012805741357\n",
            "perplexity of trigram on test with smoothing:  7361.21230049765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3 (d)\n",
        "Converting all tokens that occurred fewer than five times to \\<unk> could lower the perplexity on previously unseen data compared to solely replacing tokens that appeared only once. This strategy likely enhances the model's generalization capabilities. By substituting tokens with less than five occurrences to \\<unk>, the model gains exposure to a wider range of vocabulary during training, enabling it to better handle unseen data with a broader vocabulary spectrum.\n",
        "perplexity of unigram on test without smoothing when unk = 1:  1114.6534964214973\n",
        "\n",
        "perplexity of bigram on test without smoothing when unk = 1:   inf\n",
        "\n",
        "perplexity of trigram on test without smoothing when unk = 1:  inf\n",
        "\n",
        "perplexity of unigram on test with smoothing when unk = 1:  1115.6971793450489\n",
        "\n",
        "perplexity of bigram on test with smoothing when unk = 1:   2654.5032901347495\n",
        "\n",
        "perplexity of trigram on test with smoothing when unk = 1:  14862.456000393395\n",
        "\n",
        "perplexity of unigram on test without smoothing when unk = 5:  848.473970880849\n",
        "\n",
        "perplexity of bigram on test without smoothing when unk = 5:   inf\n",
        "\n",
        "perplexity of trigram on test without smoothing when unk = 5:  inf\n",
        "\n",
        "perplexity of unigram on test with smoothing when unk = 5:  847.7793318116068\n",
        "\n",
        "perplexity of bigram on test with smoothing when unk = 5:   1221.3500472157966\n",
        "\n",
        "perplexity of trigram on test with smoothing when unk = 5:  6402.409842898671\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C0NvTHTJ-kur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the word with frequency less than two (apper only once) to unk\n",
        "unk2 = 2\n",
        "unigram_freq_unk2 = count_for_unitoken(train_f)\n",
        "unk_list_unk2 = filter_unk(unigram_freq_unk2, unk2)\n",
        "vocab_unk2 = unigram_freq_unk2.keys()\n",
        "size_unk2 = len(vocab_unk2)\n",
        "bigram_freq_unk2 = count_for_bitoken(train_f, unk_list)\n",
        "unigram_model_unk2 = unigram(train_f, K, unk2)\n",
        "bigram_model_unk2 = bigram(train_f, K, unk2)\n",
        "trigram_model_unk2 = trigram(train_f, bigram_model_unk2, K, unk2)\n",
        "print(\"perplexity of unigram on test without smoothing when unk = 1: \", perplexity(K, 1, unigram_model_unk2, test_f, vocab_unk2, unigram_freq_unk2, bigram_freq_unk2, spec_freq))\n",
        "print(\"perplexity of bigram on test without smoothing when unk = 1:  \", perplexity(K, 2, bigram_model_unk2, test_f, vocab_unk2, unigram_freq_unk2, bigram_freq_unk2, spec_freq))\n",
        "print(\"perplexity of trigram on test without smoothing when unk = 1: \", perplexity(K, 3, trigram_model_unk2, test_f, vocab_unk2, unigram_freq_unk2, bigram_freq_unk2, spec_freq))\n",
        "print(\"perplexity of unigram on test with smoothing when unk = 1: \", perplexity_laplace(K, 1, unigram_model_unk2, test_f, vocab_unk2, unigram_freq_unk2, bigram_freq_unk2, spec_freq))\n",
        "print(\"perplexity of bigram on test with smoothing when unk = 1:  \", perplexity_laplace(K, 2, bigram_model_unk2, test_f, vocab_unk2, unigram_freq_unk2, bigram_freq_unk2, spec_freq))\n",
        "print(\"perplexity of trigram on test with smoothing when unk = 1: \", perplexity_laplace(K, 3, trigram_model_unk2, test_f, vocab_unk2, unigram_freq_unk2, bigram_freq_unk2, spec_freq))\n",
        "\n",
        "# convert the word with frequency less than five to unk\n",
        "unk5 = 5\n",
        "unigram_freq_unk5 = count_for_unitoken(train_f)\n",
        "unk_list_unk5 = filter_unk(unigram_freq_unk5, unk5)\n",
        "vocab_unk5 = unigram_freq_unk5.keys()\n",
        "size_unk5 = len(vocab_unk5)\n",
        "bigram_freq_unk5 = count_for_bitoken(train_f, unk_list)\n",
        "unigram_model_unk5 = unigram(train_f, K, unk5)\n",
        "bigram_model_unk5 = bigram(train_f, K, unk5)\n",
        "trigram_model_unk5 = trigram(train_f, bigram_model_unk5, K, unk5)\n",
        "print(\"perplexity of unigram on test without smoothing when unk = 5: \", perplexity(K, 1, unigram_model_unk5, test_f, vocab_unk5, unigram_freq_unk5, bigram_freq_unk5, spec_freq))\n",
        "print(\"perplexity of bigram on test without smoothing when unk = 5:  \", perplexity(K, 2, bigram_model_unk5, test_f, vocab_unk5, unigram_freq_unk5, bigram_freq_unk5, spec_freq))\n",
        "print(\"perplexity of trigram on test without smoothing when unk = 5: \", perplexity(K, 3, trigram_model_unk5, test_f, vocab_unk5, unigram_freq_unk5, bigram_freq_unk5, spec_freq))\n",
        "print(\"perplexity of unigram on test with smoothing when unk = 5: \", perplexity_laplace(K, 1, unigram_model_unk5, test_f, vocab_unk5, unigram_freq_unk5, bigram_freq_unk5, spec_freq))\n",
        "print(\"perplexity of bigram on test with smoothing when unk = 5:  \", perplexity_laplace(K, 2, bigram_model_unk5, test_f, vocab_unk5, unigram_freq_unk5, bigram_freq_unk5, spec_freq))\n",
        "print(\"perplexity of trigram on test with smoothing when unk = 5: \", perplexity_laplace(K, 3, trigram_model_unk5, test_f, vocab_unk5, unigram_freq_unk5, bigram_freq_unk5, spec_freq))\n"
      ],
      "metadata": {
        "id": "Kppy5rRIaz7p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc70a31d-4e43-4651-dcca-3c6f9111602b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perplexity of unigram on test without smoothing when unk = 1:  1114.6534964214973\n",
            "perplexity of bigram on test without smoothing when unk = 1:   inf\n",
            "perplexity of trigram on test without smoothing when unk = 1:  inf\n",
            "perplexity of unigram on test with smoothing when unk = 1:  1115.6971793450489\n",
            "perplexity of bigram on test with smoothing when unk = 1:   2654.5032901347495\n",
            "perplexity of trigram on test with smoothing when unk = 1:  14862.456000393395\n",
            "perplexity of unigram on test without smoothing when unk = 5:  848.473970880849\n",
            "perplexity of bigram on test without smoothing when unk = 5:   inf\n",
            "perplexity of trigram on test without smoothing when unk = 5:  inf\n",
            "perplexity of unigram on test with smoothing when unk = 5:  847.7793318116068\n",
            "perplexity of bigram on test with smoothing when unk = 5:   1221.3500472157966\n",
            "perplexity of trigram on test with smoothing when unk = 5:  6402.409842898671\n"
          ]
        }
      ]
    }
  ]
}